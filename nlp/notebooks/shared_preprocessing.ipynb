{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "958b9a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added: c:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "print(\"Project root added:\", PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d8be61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.shared.preprocessing import clean_text\n",
    "from nlp.shared.lexicon_loader import load_lexicon\n",
    "from nlp.shared.lemmatizer import lemmatize_text\n",
    "from nlp.shared.lexicon_loader import load_lexicon\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28707058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I be always tired and can not breathe well'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"OUT   of   breath!!!\")\n",
    "lemmatize_text(\"I am always tired and cannot breathe well\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1d52476",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\abdul\\\\Desktop\\\\gradproj\\\\Graduation-Project-2\\\\nlp\\\\nlp\\\\data\\\\lexicon.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 1️⃣ Load Lexicon\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m     19\u001b[39m lexicon_path = project_root / \u001b[33m\"\u001b[39m\u001b[33mnlp\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mlexicon.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m lexicon = \u001b[43mload_lexicon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlexicon_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 2️⃣ Load Disease CSVs\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ---------------------------\u001b[39;00m\n\u001b[32m     25\u001b[39m diabetes_df = pd.read_csv(project_root / \u001b[33m\"\u001b[39m\u001b[33mnlp\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mdiabetes_symptoms.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\gradproj\\Graduation-Project-2\\nlp\\shared\\lexicon_loader.py:17\u001b[39m, in \u001b[36mload_lexicon\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     14\u001b[39m     project_root = file_location.parent.parent  \u001b[38;5;66;03m# goes up from shared/ → project root\u001b[39;00m\n\u001b[32m     15\u001b[39m     path = project_root / \u001b[33m\"\u001b[39m\u001b[33mnlp\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mlexicon.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m lexicon_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcp1252\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Remove empty columns\u001b[39;00m\n\u001b[32m     20\u001b[39m lexicon_df = lexicon_df.dropna(axis=\u001b[32m1\u001b[39m, how=\u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\abdul\\\\Desktop\\\\gradproj\\\\Graduation-Project-2\\\\nlp\\\\nlp\\\\data\\\\lexicon.csv'"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# 0️⃣ Setup\n",
    "# ---------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from nlp.shared.lexicon_loader import load_lexicon\n",
    "from nlp.shared.symptom_matcher import match_symptoms\n",
    "from nlp.shared.preprocessing import clean_text\n",
    "from nlp.shared.lemmatizer import lemmatize_text\n",
    "\n",
    "# ---------------------------\n",
    "# 1️⃣ Load Lexicon\n",
    "# ---------------------------\n",
    "lexicon_path = project_root / \"nlp\" / \"data\" / \"lexicon.csv\"\n",
    "lexicon = load_lexicon(str(lexicon_path))\n",
    "\n",
    "# ---------------------------\n",
    "# 2️⃣ Load Disease CSVs\n",
    "# ---------------------------\n",
    "diabetes_df = pd.read_csv(project_root / \"nlp\" / \"data\" / \"diabetes_symptoms.csv\")\n",
    "anemia_df = pd.read_csv(project_root / \"nlp\" / \"data\" / \"anemia_symptoms.csv\")\n",
    "\n",
    "# Convert to dict: disease -> set of symptoms\n",
    "disease_symptoms = {}\n",
    "\n",
    "for df in [diabetes_df, anemia_df]:\n",
    "    for _, row in df.iterrows():\n",
    "        disease = row['disease'].strip()\n",
    "        symptoms = set([s.strip() for s in row[1:] if pd.notna(s)])\n",
    "        disease_symptoms[disease] = symptoms\n",
    "\n",
    "# ---------------------------\n",
    "# 3️⃣ Function to map patient text to disease\n",
    "# ---------------------------\n",
    "def match_disease(text, lexicon, disease_symptoms):\n",
    "    # 1️⃣ Lexicon matching\n",
    "    matched_symptoms = match_symptoms(text, lexicon)\n",
    "    lexicon_flag = len(matched_symptoms) > 0\n",
    "    \n",
    "    # 2️⃣ Disease mapping\n",
    "    results = []\n",
    "    if lexicon_flag:\n",
    "        for disease, symptoms in disease_symptoms.items():\n",
    "            matched_in_disease = set(matched_symptoms) & symptoms\n",
    "            if matched_in_disease:\n",
    "                percentage = len(matched_in_disease) / len(symptoms) * 100\n",
    "                results.append({\n",
    "                    \"disease\": disease,\n",
    "                    \"matched_symptoms\": list(matched_in_disease),\n",
    "                    \"percentage\": round(percentage, 2)\n",
    "                })\n",
    "    return results, lexicon_flag\n",
    "\n",
    "# ---------------------------\n",
    "# 4️⃣ Test Example\n",
    "# ---------------------------\n",
    "examples = [\n",
    "    \"I get worn out after small activities\",\n",
    "    \"My body shuts down after walking\",\n",
    "    \"I cut myself and the bleeding does not stop\",\n",
    "    \"I am constantly thirsty and urinate frequently\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    results, flag = match_disease(text, lexicon, disease_symptoms)\n",
    "    print(\"\\nTEXT:\", text)\n",
    "    print(\"Lexicon matched:\", flag)\n",
    "    if results:\n",
    "        for r in results:\n",
    "            print(f\"→ Disease: {r['disease']} | Symptoms: {r['matched_symptoms']} | % matched: {r['percentage']}\")\n",
    "    else:\n",
    "        print(\"→ No symptoms detected via lexicon, fallback needed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8f58a969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Symptoms: ['Fatigue', 'Shortness of breath']\n",
      "Diseases: [{'disease': 'anemia', 'matched_symptoms': ['Fatigue'], 'percentage': 10.0}]\n"
     ]
    }
   ],
   "source": [
    "from nlp.shared.lexicon_loader import load_lexicon\n",
    "from nlp.shared.symptom_matcher import match_symptoms\n",
    "from nlp.shared.disease_matcher import match_diseases\n",
    "import pandas as pd\n",
    "# Load lexicon (unchanged)\n",
    "lexicon = load_lexicon(\"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/lexicon.csv\")\n",
    "\n",
    "# Load disease files\n",
    "diabetes = pd.read_csv(\"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/diabetes_symptoms.csv\")\n",
    "anemia = pd.read_csv(\"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/anemia_symptoms.csv\")\n",
    "\n",
    "disease_symptoms = {}\n",
    "\n",
    "for df in [diabetes, anemia]:\n",
    "    for _, row in df.iterrows():\n",
    "        disease = row[\"disease\"].strip()\n",
    "        symptoms = set(s.strip() for s in row[1:] if pd.notna(s))\n",
    "        disease_symptoms[disease] = symptoms\n",
    "text = \"I feel tired and get out of breath easily\"\n",
    "\n",
    "# Step 1: lexicon\n",
    "detected_symptoms = match_symptoms(text, lexicon)\n",
    "\n",
    "# Step 2: disease matching\n",
    "results = match_diseases(detected_symptoms, disease_symptoms)\n",
    "\n",
    "print(\"Symptoms:\", detected_symptoms)\n",
    "print(\"Diseases:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d89d22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT: I am always tired and feel out of breath\n",
      "Lexicon matched: True\n",
      "→ Disease: anemia | Symptoms: ['fatigue', 'shortness of breath'] | % matched: 20.0\n",
      "→ Disease: diabetes | Symptoms: ['fatigue'] | % matched: 8.33\n",
      "\n",
      "TEXT: I cut myself and the bleeding does not stop\n",
      "Lexicon matched: False\n",
      "→ No disease matched (semantic fallback later)\n",
      "\n",
      "TEXT: I am constantly thirsty and urinate frequently\n",
      "Lexicon matched: False\n",
      "→ No disease matched (semantic fallback later)\n",
      "\n",
      "TEXT: I feel weak and dizzy\n",
      "Lexicon matched: True\n",
      "→ Disease: anemia | Symptoms: ['weakness'] | % matched: 10.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# lexicon_path = os.path.join(project_root, \"nlp\", \"data\", \"lexicon.csv\")\n",
    "# diabetes_path = os.path.join(project_root, \"nlp\", \"data\", \"diabetes_symptoms.csv\")\n",
    "# anemia_path = os.path.join(project_root, \"nlp\", \"data\", \"anemia_symptoms.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0️⃣ SETUP (so Jupyter sees nlp/)\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from nlp.shared.lexicon_loader import load_lexicon\n",
    "from nlp.shared.symptom_matcher import match_symptoms\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1️⃣ LOAD LEXICON (UNCHANGED, WORKING)\n",
    "# ============================================================\n",
    "\n",
    "lexicon_path = os.path.join(project_root, \"data\", \"lexicon.csv\")\n",
    "lexicon = load_lexicon(str(lexicon_path))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣ LOAD DISEASE CSVs\n",
    "# ============================================================\n",
    "\n",
    "diabetes_df = pd.read_csv(os.path.join(project_root, \"data\", \"diabetes_symptoms.csv\"))\n",
    "anemia_df   = pd.read_csv(os.path.join(project_root, \"data\", \"anemia_symptoms.csv\"))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣ NORMALIZATION (THIS FIXES YOUR BUG)\n",
    "# ============================================================\n",
    "\n",
    "def normalize_symptom(s: str) -> str:\n",
    "    return s.strip().lower()\n",
    "\n",
    "\n",
    "# Convert CSVs → disease → set(symptoms)\n",
    "disease_symptoms = {}\n",
    "\n",
    "for df in [diabetes_df, anemia_df]:\n",
    "    for _, row in df.iterrows():\n",
    "        disease = row[\"disease\"].strip().lower()\n",
    "        symptoms = {\n",
    "            normalize_symptom(s)\n",
    "            for s in row[1:]\n",
    "            if pd.notna(s)\n",
    "        }\n",
    "        disease_symptoms[disease] = symptoms\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣ DISEASE MATCHER (LEXICON FIRST)\n",
    "# ============================================================\n",
    "\n",
    "def match_disease(patient_text: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - results: list of diseases with symptoms + percentage\n",
    "    - lexicon_flag: True if lexicon matched anything\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Lexicon symptom matching\n",
    "    detected_symptoms = match_symptoms(patient_text, lexicon)\n",
    "    detected_symptoms = [normalize_symptom(s) for s in detected_symptoms]\n",
    "\n",
    "    lexicon_flag = len(detected_symptoms) > 0\n",
    "\n",
    "    if not lexicon_flag:\n",
    "        return [], False\n",
    "\n",
    "    detected_set = set(detected_symptoms)\n",
    "    results = []\n",
    "\n",
    "    # Step 2: Disease scoring\n",
    "    for disease, disease_set in disease_symptoms.items():\n",
    "        matched = detected_set & disease_set\n",
    "        if matched:\n",
    "            percentage = len(matched) / len(disease_set) * 100\n",
    "            results.append({\n",
    "                \"disease\": disease,\n",
    "                \"matched_symptoms\": sorted(matched),\n",
    "                \"percentage\": round(percentage, 2)\n",
    "            })\n",
    "\n",
    "    # Sort by percentage (descending)\n",
    "    results.sort(key=lambda x: x[\"percentage\"], reverse=True)\n",
    "\n",
    "    return results, True\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5️⃣ TEST CASES\n",
    "# ============================================================\n",
    "\n",
    "tests = [\n",
    "    \"I am always tired and feel out of breath\",\n",
    "    \"I cut myself and the bleeding does not stop\",\n",
    "    \"I am constantly thirsty and urinate frequently\",\n",
    "    \"I feel weak and dizzy\"\n",
    "]\n",
    "\n",
    "for text in tests:\n",
    "    results, flag = match_disease(text)\n",
    "\n",
    "    print(\"\\nTEXT:\", text)\n",
    "    print(\"Lexicon matched:\", flag)\n",
    "\n",
    "    if results:\n",
    "        for r in results:\n",
    "            print(\n",
    "                f\"→ Disease: {r['disease']} | \"\n",
    "                f\"Symptoms: {r['matched_symptoms']} | \"\n",
    "                f\"% matched: {r['percentage']}\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"→ No disease matched (semantic fallback later)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b6d7bf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT: I am always tired and feel out of breath\n",
      "Lexicon matched: True\n",
      "[{'disease': 'anemia', 'matched_symptoms': ['fatigue', 'shortness of breath'], 'percentage': 20.0}, {'disease': 'diabetes', 'matched_symptoms': ['fatigue'], 'percentage': 8.33}]\n",
      "\n",
      "TEXT: I cut myself and the bleeding does not stop\n",
      "Lexicon matched: False\n",
      "No disease matched\n",
      "\n",
      "TEXT: I am constantly thirsty and urinate frequently\n",
      "Lexicon matched: False\n",
      "No disease matched\n",
      "\n",
      "TEXT: I feel weak and dizzy\n",
      "Lexicon matched: True\n",
      "[{'disease': 'anemia', 'matched_symptoms': ['weakness'], 'percentage': 10.0}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# lexicon_path = os.path.join(project_root, \"nlp\", \"data\", \"lexicon.csv\")\n",
    "# diabetes_path = os.path.join(project_root, \"nlp\", \"data\", \"diabetes_symptoms.csv\")\n",
    "# anemia_path = os.path.join(project_root, \"nlp\", \"data\", \"anemia_symptoms.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 0️⃣ SETUP (so Jupyter sees nlp/)\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "#from nlp.shared.lexicon_loader import load_lexicon\n",
    "from nlp.shared.symptom_matcher import match_symptoms\n",
    "\n",
    "\n",
    "from nlp.shared.lexicon_loader import load_lexicon\n",
    "from shared.disease_matcher import load_disease_symptoms \n",
    "from shared.disease_matcher import match_disease\n",
    "\n",
    "\n",
    "# Load data\n",
    "\n",
    "lexicon_path = os.path.join(project_root, \"data\", \"lexicon.csv\")\n",
    "lexicon = load_lexicon(str(lexicon_path))\n",
    "\n",
    "\n",
    "diabetes_df = pd.read_csv(os.path.join(project_root, \"data\", \"diabetes_symptoms.csv\"))\n",
    "anemia_df   = pd.read_csv(os.path.join(project_root, \"data\", \"anemia_symptoms.csv\"))\n",
    "\n",
    "\n",
    "disease_symptoms = load_disease_symptoms(diabetes_df, anemia_df)\n",
    "\n",
    "tests = [\n",
    "    \"I am always tired and feel out of breath\",\n",
    "    \"I cut myself and the bleeding does not stop\",\n",
    "    \"I am constantly thirsty and urinate frequently\",\n",
    "    \"I feel weak and dizzy\"\n",
    "]\n",
    "\n",
    "for text in tests:\n",
    "    results, flag = match_disease(text, lexicon, disease_symptoms)\n",
    "    print(\"\\nTEXT:\", text)\n",
    "    print(\"Lexicon matched:\", flag)\n",
    "    print(results if results else \"No disease matched\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b303d14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89261fe6541f42febc9391b05c9d45a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT: I am always tired and feel out of breath\n",
      "Lexicon matched: True\n",
      "→ Disease: anemia | Symptoms: ['fatigue', 'shortness of breath'] | % matched: 20.0\n",
      "→ Disease: diabetes | Symptoms: ['fatigue'] | % matched: 8.33\n",
      "\n",
      "TEXT: I cut myself and the bleeding does not stop\n",
      "Lexicon matched: False\n",
      "→ Disease: anemia | Symptoms: ['prolonged bleeding'] | % matched: 10.0\n",
      "\n",
      "TEXT: I am constantly thirsty and urinate frequently\n",
      "Lexicon matched: False\n",
      "→ Disease: diabetes | Symptoms: ['frequent urination', 'increased thirst'] | % matched: 16.67\n",
      "\n",
      "TEXT: I feel weak and dizzy\n",
      "Lexicon matched: True\n",
      "→ Disease: anemia | Symptoms: ['weakness'] | % matched: 10.0\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Disease Matcher Test in Notebook\n",
    "# -----------------------------\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from shared.disease_matcher_full import DiseaseMatcher\n",
    "\n",
    "# -----------------------------\n",
    "# Paths\n",
    "# -----------------------------\n",
    "project_root = Path.cwd().parent\n",
    "lexicon_path  = os.path.join(project_root, \"data\", \"lexicon.csv\")\n",
    "diabetes_path = os.path.join(project_root, \"data\", \"diabetes_symptoms.csv\")\n",
    "anemia_path   = os.path.join(project_root, \"data\", \"anemia_symptoms.csv\")\n",
    "\n",
    "# -----------------------------\n",
    "# Initialize Matcher\n",
    "# -----------------------------\n",
    "matcher = DiseaseMatcher(\n",
    "    lexicon_path=str(lexicon_path),\n",
    "    diabetes_path=str(diabetes_path),\n",
    "    anemia_path=str(anemia_path),\n",
    "    semantic_model_name=\"all-MiniLM-L6-v2\",\n",
    "    semantic_threshold=0.55\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Test Examples\n",
    "# -----------------------------\n",
    "tests = [\n",
    "    \"I am always tired and feel out of breath\",\n",
    "    \"I cut myself and the bleeding does not stop\",\n",
    "    \"I am constantly thirsty and urinate frequently\",\n",
    "    \"I feel weak and dizzy\"\n",
    "]\n",
    "\n",
    "for text in tests:\n",
    "    results, lexicon_flag = matcher.match_disease(text)\n",
    "    print(\"\\nTEXT:\", text)\n",
    "    print(\"Lexicon matched:\", lexicon_flag)\n",
    "    if results:\n",
    "        for r in results:\n",
    "            print(f\"→ Disease: {r['disease']} | Symptoms: {r['matched_symptoms']} | % matched: {r['percentage']}\")\n",
    "    else:\n",
    "        print(\"→ No disease matched\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1315e38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I am always tired and feel out of breath',\n",
       " 'lexicon_matched': True,\n",
       " 'results': [{'disease': 'anemia',\n",
       "   'matched_symptoms': ['fatigue', 'shortness of breath'],\n",
       "   'percentage': 20.0},\n",
       "  {'disease': 'diabetes',\n",
       "   'matched_symptoms': ['fatigue'],\n",
       "   'percentage': 8.33}]}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shared.predictor import predict\n",
    "\n",
    "predict(\"I am always tired and feel out of breath\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1252dc2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Users\\\\abdul\\\\Desktop\\\\gradproj\\\\Graduation-Project-2\\\\nlp\\\\nlp\\\\data\\\\lexicon.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[83]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 1️⃣ LOAD LEXICON (UNCHANGED, WORKING)\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     20\u001b[39m lexicon_path = project_root / \u001b[33m\"\u001b[39m\u001b[33mnlp\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mlexicon.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m lexicon = \u001b[43mload_lexicon\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlexicon_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# 2️⃣ LOAD DISEASE CSVs\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[32m     28\u001b[39m diabetes_df = pd.read_csv(project_root / \u001b[33m\"\u001b[39m\u001b[33mnlp\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mdiabetes_symptoms.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\gradproj\\Graduation-Project-2\\nlp\\shared\\lexicon_loader.py:17\u001b[39m, in \u001b[36mload_lexicon\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     14\u001b[39m     project_root = file_location.parent.parent  \u001b[38;5;66;03m# goes up from shared/ → project root\u001b[39;00m\n\u001b[32m     15\u001b[39m     path = project_root / \u001b[33m\"\u001b[39m\u001b[33mnlp\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mlexicon.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m lexicon_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcp1252\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Remove empty columns\u001b[39;00m\n\u001b[32m     20\u001b[39m lexicon_df = lexicon_df.dropna(axis=\u001b[32m1\u001b[39m, how=\u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\common.py:926\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    921\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    922\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    923\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    924\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    925\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m926\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    934\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    935\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'c:\\\\Users\\\\abdul\\\\Desktop\\\\gradproj\\\\Graduation-Project-2\\\\nlp\\\\nlp\\\\data\\\\lexicon.csv'"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 0️⃣ SETUP (so Jupyter sees nlp/)\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from nlp.shared.lexicon_loader import load_lexicon\n",
    "from nlp.shared.symptom_matcher import match_symptoms\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 1️⃣ LOAD LEXICON (UNCHANGED, WORKING)\n",
    "# ============================================================\n",
    "\n",
    "lexicon_path = project_root / \"nlp\" / \"data\" / \"lexicon.csv\"\n",
    "lexicon = load_lexicon(str(lexicon_path))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 2️⃣ LOAD DISEASE CSVs\n",
    "# ============================================================\n",
    "\n",
    "diabetes_df = pd.read_csv(project_root / \"nlp\" / \"data\" / \"diabetes_symptoms.csv\")\n",
    "anemia_df   = pd.read_csv(project_root / \"nlp\" / \"data\" / \"anemia_symptoms.csv\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 3️⃣ NORMALIZATION (THIS FIXES YOUR BUG)\n",
    "# ============================================================\n",
    "\n",
    "def normalize_symptom(s: str) -> str:\n",
    "    return s.strip().lower()\n",
    "\n",
    "\n",
    "# Convert CSVs → disease → set(symptoms)\n",
    "disease_symptoms = {}\n",
    "\n",
    "for df in [diabetes_df, anemia_df]:\n",
    "    for _, row in df.iterrows():\n",
    "        disease = row[\"disease\"].strip().lower()\n",
    "        symptoms = {\n",
    "            normalize_symptom(s)\n",
    "            for s in row[1:]\n",
    "            if pd.notna(s)\n",
    "        }\n",
    "        disease_symptoms[disease] = symptoms\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 4️⃣ DISEASE MATCHER (LEXICON FIRST)\n",
    "# ============================================================\n",
    "\n",
    "def match_disease(patient_text: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    - results: list of diseases with symptoms + percentage\n",
    "    - lexicon_flag: True if lexicon matched anything\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Lexicon symptom matching\n",
    "    detected_symptoms = match_symptoms(patient_text, lexicon)\n",
    "    detected_symptoms = [normalize_symptom(s) for s in detected_symptoms]\n",
    "\n",
    "    lexicon_flag = len(detected_symptoms) > 0\n",
    "\n",
    "    if not lexicon_flag:\n",
    "        return [], False\n",
    "\n",
    "    detected_set = set(detected_symptoms)\n",
    "    results = []\n",
    "\n",
    "    # Step 2: Disease scoring\n",
    "    for disease, disease_set in disease_symptoms.items():\n",
    "        matched = detected_set & disease_set\n",
    "        if matched:\n",
    "            percentage = len(matched) / len(disease_set) * 100\n",
    "            results.append({\n",
    "                \"disease\": disease,\n",
    "                \"matched_symptoms\": sorted(matched),\n",
    "                \"percentage\": round(percentage, 2)\n",
    "            })\n",
    "\n",
    "    # Sort by percentage (descending)\n",
    "    results.sort(key=lambda x: x[\"percentage\"], reverse=True)\n",
    "\n",
    "    return results, True\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5️⃣ TEST CASES\n",
    "# ============================================================\n",
    "\n",
    "tests = [\n",
    "    \"I am always tired and feel out of breath\",\n",
    "    \"I cut myself and the bleeding does not stop\",\n",
    "    \"I am constantly thirsty and urinate frequently\",\n",
    "    \"I feel weak and dizzy\"\n",
    "]\n",
    "\n",
    "for text in tests:\n",
    "    results, flag = match_disease(text)\n",
    "\n",
    "    print(\"\\nTEXT:\", text)\n",
    "    print(\"Lexicon matched:\", flag)\n",
    "\n",
    "    if results:\n",
    "        for r in results:\n",
    "            print(\n",
    "                f\"→ Disease: {r['disease']} | \"\n",
    "                f\"Symptoms: {r['matched_symptoms']} | \"\n",
    "                f\"% matched: {r['percentage']}\"\n",
    "            )\n",
    "    else:\n",
    "        print(\"→ No disease matched (semantic fallback later)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffe7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fatigue: ['tired', 'exhaust', 'drain', 'no energy', 'always sleepy', 'feel weak all the time']\n",
      "Shortness of breath: ['breathless', 'hard to breathe', 'can not breathe well', 'out of breath', 'breathe difficulty']\n",
      "Pale skin: ['skin look pale', 'face be pale', 'lose color', 'light than usual', 'pale face']\n",
      "Weakness: ['feel weak', 'weak body', 'no strength', 'shaky', 'muscle feel weak']\n",
      "Frequent infections: ['get sick often', 'catch infection easily', 'always get ill', 'frequent illness']\n",
      "Rapid or irregular heartbeats: ['fast heartbeat', 'heart racing', 'irregular heartbeat', 'heart beat fast', 'palpitation']\n",
      "Prolonged bleeding: ['bleeding take long to stop', 'blee a lot from small cut', 'slow blood clotting', 'long bleed time']\n",
      "Bruising: ['bruise easily', 'bruise appear quickly', 'unexplained bruise', 'bruise without injury']\n",
      "Jaundice: ['yellow skin', 'yellow eye', 'eye look yellow', 'skin turn yellow']\n",
      "Decreased exercise tolerance: ['get tired quickly when walk', 'can not exercise much', 'feel exhausted after small effort', 'low stamina']\n",
      "Weight loss: ['lose weight', 'drop weight', 'get thin', 'weight go down', 'lose weight without try']\n",
      "Restlessness: ['can not relax', 'feel restless', 'uneasy feeling', 'always move', 'nervous for no reason']\n",
      "Lethargy: ['very slow', 'no motivation', 'feel lazy', 'heavy body', 'lack of activity']\n",
      "Irregular sugar level: ['sugar level not stable', 'blood sugar go up and down', 'high sugar sometimes', 'low sugar sometimes']\n",
      "Blurred and distorted vision: ['blurry vision', 'vision not clear', 'hard to see clearly', 'eyesight get bad', 'distort vision']\n",
      "Obesity: ['overweight', 'gain a lot of weight', 'very heavy', 'obese', 'weight gain']\n",
      "Excessive hunger: ['always hungry', 'eat a lot', 'feel hungry all the time', 'never feel full']\n",
      "Increased thirst: ['always thirsty', 'drink water a lot', 'mouth always dry', 'can not stop drink water']\n",
      "Polyuria: ['pee a lot', 'go to bathroom often', 'frequent urination', 'urinate many time', 'wake up at night to pee']\n",
      "Frequent urination: ['pee a lot', 'go to bathroom often', 'frequent urination', 'urinate many time', 'wake up at night to pee']\n",
      "Slow healing of wounds: ['wound heal slowly', 'cut take long to heal', 'slow recovery', 'injury heal slowly']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "from nlp.shared import lexicon_loader\n",
    "\n",
    "importlib.reload(lexicon_loader)  # reload the updated file\n",
    "\n",
    "lexicon = lexicon_loader.load_lexicon(\"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/lexicon.csv\")\n",
    "for canonical, expressions in lexicon.items():\n",
    "    print(f\"{canonical}: {expressions}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1beb2c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Symptoms: ['Fatigue', 'Shortness of breath']\n"
     ]
    }
   ],
   "source": [
    "from nlp.shared.lexicon_loader import load_lexicon\n",
    "from nlp.shared.symptom_matcher import match_symptoms\n",
    "\n",
    "# Load lexicon\n",
    "lexicon = load_lexicon(r\"C:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\nlp\\data\\lexicon.csv\")\n",
    "\n",
    "# Example patient text\n",
    "text = \"I am always tired 😴 and can’t breathe well!\"\n",
    "\n",
    "# Match symptoms\n",
    "detected = match_symptoms(text, lexicon)\n",
    "print(\"Detected Symptoms:\", detected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e10d5827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f908901fa4694bffb338d5b1019cbcde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Walking for a few minutes makes my body shut down\n",
      "Detected symptoms: []\n",
      "\n",
      "Text: I struggle to finish simple tasks without resting\n",
      "Detected symptoms: []\n",
      "\n",
      "Text: Climbing stairs feels like my lungs stop working\n",
      "Detected symptoms: []\n",
      "\n",
      "Text: After minimal effort, I feel completely worn out\n",
      "Detected symptoms: []\n",
      "\n",
      "Text: I need frequent pauses just to catch air\n",
      "Detected symptoms: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlp.shared.semantic_utils import semantic_expand\n",
    "\n",
    "canonical_symptoms = [\n",
    "    \"Fatigue\",\n",
    "    \"Shortness of breath\"\n",
    "]\n",
    "\n",
    "test_texts = [\n",
    "    \"Walking for a few minutes makes my body shut down\",\n",
    "    \"I struggle to finish simple tasks without resting\",\n",
    "    \"Climbing stairs feels like my lungs stop working\",\n",
    "    \"After minimal effort, I feel completely worn out\",\n",
    "    \"I need frequent pauses just to catch air\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    detected = semantic_expand(text, canonical_symptoms)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Detected symptoms: {detected}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "03284e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get worn out after small activities\n",
      "[]\n",
      "\n",
      "My body shuts down after walking\n",
      "[]\n",
      "\n",
      "I struggle to climb stairs without stopping\n",
      "[]\n",
      "\n",
      "Even minor effort leaves me exhausted\n",
      "[]\n",
      "\n",
      "I need pauses just to get enough air\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "canonical_symptoms = [\n",
    "    \"Fatigue\",\n",
    "    \"Shortness of breath\"\n",
    "]\n",
    "\n",
    "texts = [\n",
    "    \"I get worn out after small activities\",\n",
    "    \"My body shuts down after walking\",\n",
    "    \"I struggle to climb stairs without stopping\",\n",
    "    \"Even minor effort leaves me exhausted\",\n",
    "    \"I need pauses just to get enough air\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    print(text)\n",
    "    print(semantic_expand(text, canonical_symptoms))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef883a9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dict',\n",
       " 'List',\n",
       " 'SemanticMatcher',\n",
       " 'SentenceTransformer',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '__warningregistry__',\n",
       " 'cosine',\n",
       " 'pd',\n",
       " 'spacy',\n",
       " 'util']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import nlp.shared.semantic_utils as semantic_utils\n",
    "\n",
    "importlib.reload(semantic_utils)\n",
    "\n",
    "dir(semantic_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18b58e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.shared.semantic_utils import semantic_match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "040f98e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get worn out after small activities\n",
      "[{'symptom': 'Fatigue', 'score': 0.781}, {'symptom': 'Shortness of breath', 'score': 0.791}, {'symptom': 'Pale skin', 'score': 0.601}, {'symptom': 'Weakness', 'score': 0.749}, {'symptom': 'Frequent infections', 'score': 0.899}, {'symptom': 'Rapid or irregular heartbeats', 'score': 0.607}, {'symptom': 'Prolonged bleeding', 'score': 0.806}, {'symptom': 'Bruising', 'score': 0.796}, {'symptom': 'Jaundice', 'score': 0.638}, {'symptom': 'Decreased exercise tolerance', 'score': 0.855}, {'symptom': 'Weight loss', 'score': 0.717}, {'symptom': 'Restlessness', 'score': 0.792}, {'symptom': 'Lethargy', 'score': 0.754}, {'symptom': 'Irregular sugar level', 'score': 0.73}, {'symptom': 'Blurred and distorted vision', 'score': 0.774}, {'symptom': 'Obesity', 'score': 0.686}, {'symptom': 'Excessive hunger', 'score': 0.798}, {'symptom': 'Increased thirst', 'score': 0.781}, {'symptom': 'Polyuria', 'score': 0.843}, {'symptom': 'Frequent urination', 'score': 0.843}, {'symptom': 'Slow healing of wounds', 'score': 0.667}]\n",
      "\n",
      "My body shuts down after walking\n",
      "[{'symptom': 'Fatigue', 'score': 0.845}, {'symptom': 'Shortness of breath', 'score': 0.79}, {'symptom': 'Weakness', 'score': 0.795}, {'symptom': 'Frequent infections', 'score': 0.798}, {'symptom': 'Prolonged bleeding', 'score': 0.772}, {'symptom': 'Bruising', 'score': 0.805}, {'symptom': 'Decreased exercise tolerance', 'score': 0.827}, {'symptom': 'Weight loss', 'score': 0.726}, {'symptom': 'Restlessness', 'score': 0.834}, {'symptom': 'Lethargy', 'score': 0.767}, {'symptom': 'Irregular sugar level', 'score': 0.784}, {'symptom': 'Blurred and distorted vision', 'score': 0.77}, {'symptom': 'Obesity', 'score': 0.656}, {'symptom': 'Excessive hunger', 'score': 0.815}, {'symptom': 'Increased thirst', 'score': 0.731}, {'symptom': 'Polyuria', 'score': 0.779}, {'symptom': 'Frequent urination', 'score': 0.779}, {'symptom': 'Slow healing of wounds', 'score': 0.757}]\n",
      "\n",
      "I struggle to climb stairs without stopping\n",
      "[{'symptom': 'Fatigue', 'score': 0.804}, {'symptom': 'Shortness of breath', 'score': 0.77}, {'symptom': 'Weakness', 'score': 0.754}, {'symptom': 'Frequent infections', 'score': 0.769}, {'symptom': 'Rapid or irregular heartbeats', 'score': 0.645}, {'symptom': 'Prolonged bleeding', 'score': 0.739}, {'symptom': 'Bruising', 'score': 0.764}, {'symptom': 'Decreased exercise tolerance', 'score': 0.834}, {'symptom': 'Weight loss', 'score': 0.694}, {'symptom': 'Restlessness', 'score': 0.839}, {'symptom': 'Lethargy', 'score': 0.788}, {'symptom': 'Irregular sugar level', 'score': 0.641}, {'symptom': 'Blurred and distorted vision', 'score': 0.82}, {'symptom': 'Obesity', 'score': 0.64}, {'symptom': 'Excessive hunger', 'score': 0.807}, {'symptom': 'Increased thirst', 'score': 0.726}, {'symptom': 'Polyuria', 'score': 0.81}, {'symptom': 'Frequent urination', 'score': 0.81}, {'symptom': 'Slow healing of wounds', 'score': 0.696}]\n",
      "\n",
      "Even minor effort leaves me depleted\n",
      "[{'symptom': 'Fatigue', 'score': 0.745}, {'symptom': 'Shortness of breath', 'score': 0.749}, {'symptom': 'Weakness', 'score': 0.716}, {'symptom': 'Frequent infections', 'score': 0.7}, {'symptom': 'Rapid or irregular heartbeats', 'score': 0.648}, {'symptom': 'Prolonged bleeding', 'score': 0.745}, {'symptom': 'Bruising', 'score': 0.717}, {'symptom': 'Decreased exercise tolerance', 'score': 0.739}, {'symptom': 'Weight loss', 'score': 0.687}, {'symptom': 'Restlessness', 'score': 0.723}, {'symptom': 'Lethargy', 'score': 0.758}, {'symptom': 'Irregular sugar level', 'score': 0.759}, {'symptom': 'Blurred and distorted vision', 'score': 0.732}, {'symptom': 'Obesity', 'score': 0.709}, {'symptom': 'Excessive hunger', 'score': 0.715}, {'symptom': 'Increased thirst', 'score': 0.751}, {'symptom': 'Polyuria', 'score': 0.698}, {'symptom': 'Frequent urination', 'score': 0.698}, {'symptom': 'Slow healing of wounds', 'score': 0.684}]\n",
      "\n",
      "I need pauses just to get enough air\n",
      "[{'symptom': 'Fatigue', 'score': 0.839}, {'symptom': 'Shortness of breath', 'score': 0.817}, {'symptom': 'Pale skin', 'score': 0.608}, {'symptom': 'Weakness', 'score': 0.772}, {'symptom': 'Frequent infections', 'score': 0.851}, {'symptom': 'Prolonged bleeding', 'score': 0.809}, {'symptom': 'Bruising', 'score': 0.806}, {'symptom': 'Decreased exercise tolerance', 'score': 0.884}, {'symptom': 'Weight loss', 'score': 0.796}, {'symptom': 'Restlessness', 'score': 0.866}, {'symptom': 'Lethargy', 'score': 0.781}, {'symptom': 'Irregular sugar level', 'score': 0.759}, {'symptom': 'Blurred and distorted vision', 'score': 0.877}, {'symptom': 'Obesity', 'score': 0.717}, {'symptom': 'Excessive hunger', 'score': 0.829}, {'symptom': 'Increased thirst', 'score': 0.752}, {'symptom': 'Polyuria', 'score': 0.87}, {'symptom': 'Frequent urination', 'score': 0.87}, {'symptom': 'Slow healing of wounds', 'score': 0.746}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlp.shared.lexicon_loader import load_lexicon\n",
    "\n",
    "lexicon = load_lexicon(\"C:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/lexicon.csv\")\n",
    "test_sentences = [\n",
    "    \"I get worn out after small activities\",\n",
    "    \"My body shuts down after walking\",\n",
    "    \"I struggle to climb stairs without stopping\",\n",
    "    \"Even minor effort leaves me depleted\",\n",
    "    \"I need pauses just to get enough air\"\n",
    "]\n",
    "\n",
    "for s in test_sentences:\n",
    "    print(s)\n",
    "    print(semantic_match(s, lexicon))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4d9a02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/lexicon.csv\"))  # Should print True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71b2d20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\nlp\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1e962335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get worn out after small activities\n",
      "[{'symptom': 'Prolonged bleeding', 'score': 0.797}, {'symptom': 'Shortness of breath', 'score': 0.776}, {'symptom': 'Fatigue', 'score': 0.744}, {'symptom': 'Weakness', 'score': 0.605}, {'symptom': 'Decreased exercise tolerance', 'score': 0.605}]\n",
      "\n",
      "My body shuts down after walking\n",
      "[{'symptom': 'Shortness of breath', 'score': 0.829}, {'symptom': 'Prolonged bleeding', 'score': 0.783}, {'symptom': 'Fatigue', 'score': 0.773}, {'symptom': 'Decreased exercise tolerance', 'score': 0.689}]\n",
      "\n",
      "I need pauses just to get enough air\n",
      "[{'symptom': 'Shortness of breath', 'score': 0.905}, {'symptom': 'Prolonged bleeding', 'score': 0.813}, {'symptom': 'Fatigue', 'score': 0.746}, {'symptom': 'Decreased exercise tolerance', 'score': 0.71}, {'symptom': 'Weakness', 'score': 0.61}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlp.shared.semantic_utils import semantic_match\n",
    "\n",
    "tests = [\n",
    "    \"I get worn out after small activities\",\n",
    "    \"My body shuts down after walking\",\n",
    "    \"I need pauses just to get enough air\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t)\n",
    "    print(semantic_match(t))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d817c9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get worn out after small activities\n",
      "[{'symptom': 'Shortness of breath', 'score': 0.776}, {'symptom': 'Blurred and distorted vision', 'score': 0.601}]\n",
      "\n",
      "My body shuts down after walking\n",
      "[{'symptom': 'Shortness of breath', 'score': 0.829}, {'symptom': 'Weight loss', 'score': 0.69}, {'symptom': 'Obesity', 'score': 0.69}]\n",
      "\n",
      "I need pauses just to get enough air\n",
      "[{'symptom': 'Shortness of breath', 'score': 0.905}, {'symptom': 'Blurred and distorted vision', 'score': 0.649}]\n",
      "\n",
      "I cut myself and the bleeding does not stop\n",
      "[{'symptom': 'Shortness of breath', 'score': 0.829}, {'symptom': 'Fatigue', 'score': 0.679}, {'symptom': 'Weakness', 'score': 0.679}]\n",
      "\n",
      "I am constantly thirsty and urinate frequently\n",
      "[{'symptom': 'Shortness of breath', 'score': 0.799}, {'symptom': 'Fatigue', 'score': 0.705}, {'symptom': 'Weakness', 'score': 0.705}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nlp.shared.semantic_utils import SemanticMatcher\n",
    "\n",
    "# Initialize matcher with your CSV paths\n",
    "matcher = SemanticMatcher(\n",
    "    types_csv_path=\"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_types.csv\",\n",
    "    map_csv_path=\"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_type_map.csv\"\n",
    ")\n",
    "\n",
    "tests = [\n",
    "    \"I get worn out after small activities\",\n",
    "    \"My body shuts down after walking\",\n",
    "    \"I need pauses just to get enough air\",\n",
    "    \"I cut myself and the bleeding does not stop\",\n",
    "    \"I am constantly thirsty and urinate frequently\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t)\n",
    "    print(matcher.semantic_match(t))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f2b51a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# test_semantic.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnlp\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mshared\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msemantic_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SemanticMatcher\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Initialize matcher with CSV files\u001b[39;00m\n\u001b[32m      6\u001b[39m matcher = SemanticMatcher(\n\u001b[32m      7\u001b[39m     types_csv_path=\u001b[33m\"\u001b[39m\u001b[33mc:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_types.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     map_csv_path=\u001b[33m\"\u001b[39m\u001b[33mc:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_type_map.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      9\u001b[39m )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nlp'"
     ]
    }
   ],
   "source": [
    "# test_semantic.py\n",
    "\n",
    "from nlp.shared.semantic_utils import SemanticMatcher\n",
    "\n",
    "# Initialize matcher with CSV files\n",
    "matcher = SemanticMatcher(\n",
    "    types_csv_path=\"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_types.csv\",\n",
    "    map_csv_path=\"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_type_map.csv\"\n",
    ")\n",
    "\n",
    "# Test sentences (out of lexicon)\n",
    "tests = [\n",
    "    \"I get worn out after small activities\",\n",
    "    \"My body shuts down after walking\",\n",
    "    \"I need pauses just to get enough air\",\n",
    "    \"I cut myself and the bleeding does not stop\",\n",
    "    \"I am constantly thirsty and urinate frequently\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t)\n",
    "    print(matcher.match_symptoms(t))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be816447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "print(nlp(\"hello\").vector.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a0f1a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from nlp.shared import semantic_utils\n",
    "\n",
    "importlib.reload(semantic_utils)\n",
    "from nlp.shared.semantic_utils import SemanticMatcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c97e6de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I get worn out after small activities\n",
      "[]\n",
      "\n",
      "My body shuts down after walking\n",
      "[{'symptom': 'Shortness of breath', 'type': 'RESPIRATORY', 'score': 0.627}]\n",
      "\n",
      "I need pauses just to get enough air\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\nlp\\shared\\semantic_utils.py:66: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  score = doc.similarity(symptom_doc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n",
      "I cut myself and the bleeding does not stop\n",
      "[{'symptom': 'Slow healing of wounds', 'type': 'FUNCTIONAL', 'score': 0.71}, {'symptom': 'Shortness of breath', 'type': 'RESPIRATORY', 'score': 0.611}]\n",
      "\n",
      "I am constantly thirsty and urinate frequently\n",
      "[{'symptom': 'Slow healing of wounds', 'type': 'FUNCTIONAL', 'score': 0.653}, {'symptom': 'Rapid or irregular heartbeats', 'type': 'FUNCTIONAL', 'score': 0.612}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "matcher = SemanticMatcher(\n",
    "    \"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_types.csv\",\n",
    "    \"c:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_type_map.csv\"\n",
    ")\n",
    "\n",
    "tests = [\n",
    "    \"I get worn out after small activities\",\n",
    "    \"My body shuts down after walking\",\n",
    "    \"I need pauses just to get enough air\",\n",
    "    \"I cut myself and the bleeding does not stop\",\n",
    "    \"I am constantly thirsty and urinate frequently\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(t)\n",
    "    print(matcher.match(t))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37159bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from nlp.shared import semantic_utils\n",
    "importlib.reload(semantic_utils)\n",
    "\n",
    "from nlp.shared.semantic_utils import SemanticMatcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15b8652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:/Users/abdul/Desktop/gradproj/Graduation-Project-2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00e34ee0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'semantic_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# test_semantic.py\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msemantic_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SemanticMatcher\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_tests\u001b[39m():\n\u001b[32m      5\u001b[39m     matcher = SemanticMatcher(\n\u001b[32m      6\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mC:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_types.csv\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mC:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_type_map.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m     )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'semantic_utils'"
     ]
    }
   ],
   "source": [
    "# test_semantic.py\n",
    "from semantic_utils import SemanticMatcher\n",
    "\n",
    "def run_tests():\n",
    "    matcher = SemanticMatcher(\n",
    "        \"C:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_types.csv\",\n",
    "        \"C:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_type_map.csv\"\n",
    "    )\n",
    "\n",
    "    tests = [\n",
    "        \"I get worn out after small activities\",\n",
    "        \"My body shuts down after walking\",\n",
    "        \"I need pauses just to get enough air\",\n",
    "        \"I cut myself and the bleeding does not stop\",\n",
    "        \"I am constantly thirsty and urinate frequently\"\n",
    "    ]\n",
    "\n",
    "    for t in tests:\n",
    "        print(\"\\nTEXT:\", t)\n",
    "        results = matcher.match(t)\n",
    "        if not results:\n",
    "            print(\"→ NO MATCH\")\n",
    "        else:\n",
    "            for r in results:\n",
    "                print(f\"→ {r['symptom']} | {r['type']} | score={r['score']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tests()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766265d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55cc0951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root added: c:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# notebook is in: nlp/notebooks/\n",
    "PROJECT_ROOT = Path.cwd().parents[1]   # Graduation-Project-2\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(\"Project root added:\", PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "956f149b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp.shared.semantic_utils import SemanticMatcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a9a54e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Symptom', 'Type'], dtype='str')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_type_map.csv\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13d86422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216197b561c24565969cf7ed58ddccdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT: I get exhausted after minimal effort\n",
      "→ TYPE: RESPIRATORY | score=0.491\n",
      "  Symptoms: ['Shortness of breath']\n",
      "→ TYPE: SENSATION | score=0.455\n",
      "  Symptoms: ['Fatigue', 'Weakness']\n",
      "\n",
      "TEXT: I need to stop to catch my breath\n",
      "→ TYPE: RESPIRATORY | score=0.432\n",
      "  Symptoms: ['Shortness of breath']\n",
      "\n",
      "TEXT: Bleeding does not stop after small cuts\n",
      "\n",
      "TEXT: I feel thirsty all the time and pee a lot\n",
      "→ TYPE: METABOLIC | score=0.61\n",
      "  Symptoms: ['Irregular sugar level', 'Excessive hunger', 'Increased thirst', 'Polyuria', 'Frequent urination', 'Frequent infections']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parents[1]\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from nlp.shared.semantic_utils import SemanticMatcher\n",
    "\n",
    "matcher = SemanticMatcher(\n",
    "    \"C:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_types.csv\",\n",
    "    \"C:/Users/abdul/Desktop/gradproj/Graduation-Project-2/nlp/data/symptom_type_map.csv\"\n",
    ")\n",
    "\n",
    "tests = [\n",
    "    \"I get exhausted after minimal effort\",\n",
    "    \"I need to stop to catch my breath\",\n",
    "    \"Bleeding does not stop after small cuts\",\n",
    "    \"I feel thirsty all the time and pee a lot\"\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(\"\\nTEXT:\", t)\n",
    "    for r in matcher.match(t):\n",
    "        print(f\"→ TYPE: {r['type']} | score={r['score']}\")\n",
    "        print(f\"  Symptoms: {r['symptoms']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90903c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7c8c4548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "PROJECT_ROOT = os.path.abspath(\"..\")  # goes from notebooks → nlp\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "aed4f623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.semantic_utils import SemanticMatcher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "158c1ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192d3238deb54a18b3bf77d2f397e2e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "matcher = SemanticMatcher(\n",
    "    symptom_types_csv=\"../data/symptom_types.csv\",\n",
    "    symptom_map_csv=\"../data/symptom_type_map.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "340dcd0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'symptom': 'Fatigue', 'type': 'SENSATION', 'score': 0.603}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matcher.match(\"I feel tired\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3f0a5841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEXT: I get worn out after small activities\n",
      "→ Decreased exercise tolerance | FUNCTIONAL | score=0.56\n",
      "\n",
      "TEXT: I need to stop to catch my breath\n",
      "→ No symptoms detected\n",
      "\n",
      "TEXT: Bleeding does not stop after small cuts\n",
      "→ No symptoms detected\n",
      "\n",
      "TEXT: I feel thirsty all the time and pee a lot\n",
      "→ Increased thirst | METABOLIC | score=0.697\n",
      "→ Frequent urination | METABOLIC | score=0.559\n",
      "\n",
      "TEXT: I feel weak and dizzy\n",
      "→ No symptoms detected\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"I get worn out after small activities\",\n",
    "    \"I need to stop to catch my breath\",\n",
    "    \"Bleeding does not stop after small cuts\",\n",
    "    \"I feel thirsty all the time and pee a lot\",\n",
    "    \"I feel weak and dizzy\",\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(\"\\nTEXT:\", t)\n",
    "    results = matcher.match(t)\n",
    "    if not results:\n",
    "        print(\"→ No symptoms detected\")\n",
    "    for r in results:\n",
    "        print(f\"→ {r['symptom']} | {r['type']} | score={r['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebf3c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76b39f174fa14595b0540cb137213a85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'float'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[77]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Make sure you give the correct path to your CSV\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m matcher = \u001b[43mSemanticMatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/symptom_type_map.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.55\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m tests = [\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI get worn out after small activities\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI need to stop to catch my breath\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mI feel weak and dizzy\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m ]\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tests:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\nlp\\shared\\semantic_utils.py:25\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, symptom_types_csv, symptom_map_csv, model_name, type_threshold, symptom_threshold, top_types, top_symptoms)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmatch\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[32m     24\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[33;03m    Returns a list of matching symptoms with type and score\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m     text_emb = \u001b[38;5;28mself\u001b[39m.model.encode([text], convert_to_tensor=\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[32m0\u001b[39m]\n\u001b[32m     28\u001b[39m     results = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:873\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, skip_blank_lines, parse_dates, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m    861\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m    862\u001b[39m     dialect,\n\u001b[32m    863\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    869\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m    870\u001b[39m )\n\u001b[32m    871\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:300\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    297\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    299\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m300\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1645\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1642\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1644\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1904\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1902\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1903\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1904\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1911\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1915\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\common.py:772\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    769\u001b[39m     codecs.lookup_error(errors)\n\u001b[32m    771\u001b[39m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m772\u001b[39m ioargs = \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    773\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    774\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    775\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    776\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    777\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    778\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    780\u001b[39m handle = ioargs.filepath_or_buffer\n\u001b[32m    781\u001b[39m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\abdul\\Desktop\\gradproj\\Graduation-Project-2\\venv\\Lib\\site-packages\\pandas\\io\\common.py:492\u001b[39m, in \u001b[36m_get_filepath_or_buffer\u001b[39m\u001b[34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[32m    489\u001b[39m     \u001b[38;5;28mhasattr\u001b[39m(filepath_or_buffer, \u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filepath_or_buffer, \u001b[33m\"\u001b[39m\u001b[33mwrite\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    490\u001b[39m ):\n\u001b[32m    491\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid file path or buffer object type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(filepath_or_buffer)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m IOArgs(\n\u001b[32m    495\u001b[39m     filepath_or_buffer=filepath_or_buffer,\n\u001b[32m    496\u001b[39m     encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    499\u001b[39m     mode=mode,\n\u001b[32m    500\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Invalid file path or buffer object type: <class 'float'>"
     ]
    }
   ],
   "source": [
    "# Make sure you give the correct path to your CSV\n",
    "matcher = SemanticMatcher(\"../data/symptom_type_map.csv\", threshold=0.55)\n",
    "\n",
    "tests = [\n",
    "    \"I get worn out after small activities\",\n",
    "    \"I need to stop to catch my breath\",\n",
    "    \"Bleeding does not stop after small cuts\",\n",
    "    \"I feel thirsty all the time and pee a lot\",\n",
    "    \"I feel weak and dizzy\",\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    print(\"\\nTEXT:\", t)\n",
    "    res = matcher.match(t)\n",
    "    if not res:\n",
    "        print(\"→ No symptoms detected\")\n",
    "    for r in res:\n",
    "        print(f\"→ {r['symptom']} | {r['type']} | score={r['score']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "96f15cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticMatcher exists in current namespace\n",
      "Module: shared.semantic_utils\n"
     ]
    }
   ],
   "source": [
    "# Search for it\n",
    "import sys\n",
    "if 'SemanticMatcher' in dir():\n",
    "    print(\"SemanticMatcher exists in current namespace\")\n",
    "    \n",
    "# Check if it's imported from somewhere\n",
    "if hasattr(SemanticMatcher, '__module__'):\n",
    "    print(f\"Module: {SemanticMatcher.__module__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a55a90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
